---
title: "Predicting Graduation Rates"
output: github_document
---

## Loaded packages
```{r}
library(tidyverse)
library(tidymodels)
```

## Directory structure and file names/paths
```{r}
## top-level directory; everything else is relative to here
tldir <- file.path("..", "..")

## data directories
rddir <- file.path(tldir, "data", "raw")
cddir <- file.path(tldir, "data", "cleaned")

## figure directory
fgdir <- file.path(tldir, "figures")

## ELS student file: zip and rds
els_zip <- "ELS_2002-12_PETS_v1_0_Student_CSV_Datasets.zip"
els_url <- file.path("https://nces.ed.gov/EDAT/Data/Zip", els_zip)
els_rds <- "els.rds"
results_rds<-"results.rds"
```
```{r}
## knitr settings
knitr::opts_chunk$set(fig.path = file.path(fgdir, "/"))
```

## Utility functions and macros
```{r}
## ------------
## functions
## ------------

## quick paste functions
`%+%` <- function(a, b) paste(a, b, sep = "")   # a %+% b --> ab
`%_%` <- function(a, b) paste(a, b, sep = "_")  # a %_% b --> a_b
`%.%` <- function(a, b) paste(a, b, sep = ".")  # a %.% b --> a.b

## are unique values of x < n (default = 20) ? TRUE : FALSE
few_unique <- function(x, n = 20) { ifelse(length(unique(x)) < n, TRUE, FALSE) }

## does x %in% val ? NA : x
recode_missing <- function(x, val) { ifelse(x %in% val, NA, x) }

## ------------
## macros
## ------------

## unpack the ELS zip file regardless of whether RDS exists? TRUE : FALSE
unzip_again <- FALSE

## missing values used across ELS variables
## CAUTION: these are appropriate for variables we select, but may not
## be for all ELS variables; change as appropriate
els_missing_vals <- c(-1, -3, -4, -8, -9)

## validation data set training proportion (between 0 and 1)
train_split <- 0.8

## bootstrap repetitions
boot_reps <- 1000
```

We need the full student-level Education Longitudinal Study of 2002
(ELS) data file. We can download a zipped version from [NCES
EDAT](https://nces.ed.gov/EDAT/Data/Zip/ELS_2002-12_PETS_v1_0_Student_CSV_Datasets.zip).
Once downloaded, the compressed file needs to be unzipped up in the
`raw` data directory. 

This code first checks for the working data file. If the working data
file is found, the script moves forward. If it's missing, the code
next checks for the zipped file, downloading it if necessary. Once the
zipped file is present, it is unpacked and the working data file
created.

```{r}
## variables we want to select from full ELS dataset, also setting
## data type for faster input since ELS is much larger than we need
keepvars <- c(
    "STU_ID",        # student id
    "STRAT_ID",      # stratum id
    "PSU",           # primary sampling unit
    "BYRACE",        # (base year) race/ethnicity 
    "BYINCOME",      # (base year) parental income
    "BYPARED",       # (base year) parental education
    "BYNELS2M",      # (base year) math score
    "BYNELS2R",      # (base year) reading score
    "F3ATTAINMENT",  # (3rd follow up) attainment
    "F2PS1SEC"       # (2nd follow up) first institution attended
)

## check if working (RDS) file exists
if (unzip_again || !file.exists(file.path(cddir, els_rds))) {
    ## ...if not, check if working (RDS) file exists
    if (!file.exists(file.path(rddir, els_zip))) {
        ## ...if not, download zip file from NCES
        download.file(url = els_url,
                      destfile = file.path(rddir, els_zip),
                      mode = "wb")
    }
    ## ...if so (or after download), unzip and munge
    df <- read_csv(file.path(rddir, els_zip)) %>%
        ## subset to variables in vector above
        select(one_of(keepvars)) %>%
        ## lower variable names
        rename_all(tolower)
    ## save for the future
    saveRDS(df, file = file.path(cddir, els_rds))
} else {
    ## ...if exists, just read in
    df <- readRDS(file = file.path(cddir, els_rds))
}
```

## Munging 
```{r}
df <- df %>%
    ## limit to students to started at four-year institutions
    filter(f2ps1sec %in% c(1:3)) %>% 
    ## recode missing values to proper NA
    mutate_all( ~ recode_missing(.x, els_missing_vals)) %>%
    ## create variable for completing a BA/BS
    mutate(ba_complete = ifelse(f3attainment %in% c(6,7,8,10), 1, 0))
```

## Average bachelor's completion overall
```{r}
df %>% pull(ba_complete) %>% mean
```

## Split to analysis data

Selecting just the variables desired for the analysis dataset. 

As with most NCES datasets, this one is almost entirely categorical
variables. After selected the variables we want, the code below
creates a character vector of the names of variables that have fewer
than 20 unique values, then mutates those to be factors.

```{r}
## create analysis data
df_a <- df %>%
    ## select variables of interest
    select(ba_complete,
           starts_with("by")) %>% 
    ## quickly identify and transform likely factor variables
    mutate_if(.,
              .predicate = function(x) few_unique(x),
              .funs = list(as_factor)) %>%
    ## better factor levels for outcome
    mutate(ba_complete = recode_factor(ba_complete,
                                      "1" = "Completed",
                                      "0" = "Incomplete"))

## get vector of likely factors for later analysis
likely_factors <- df_a %>%
    select(-ba_complete) %>% 
    select_if(~ is.factor(.)) %>%
    names

```

## Structure for resampling

For resampling, we'll use the processed outlined in ["Recipes with
`rsample`"](https://tidymodels.github.io/rsample/articles/Applications/Recipes_and_rsample.html). Our
dataset is restructured for `r train_split * 100`/`r (1 - train_split) * 100` split, with `r boot_reps`.

```{r}
## create validation data set with 
validation_data <- mc_cv(df_a, split = train_split, times = boot_reps)
```

## Recipe

Formula is just the dependent variable (DV) on the lefthand side and
everything else on the righthand side.

```{r}
## set formula
ba_complete_formula <- formula("ba_complete ~ .")

## NB: Alternatively, a recipe object can be created by first
## specifying which variables in a data set should be used and then
## sequentially defining their roles.

recipe_function <- function(df, formula, likely_factors) {
    ## PURPOSE
    ## This function takes a dataset, formula, and likely factors; then
    ## 
    ## (1) imputes
    ## (2) converts likely factors
    ## (3) centers and scales predictors

    ## df: data frame 
    ## formula: formula object
    ## likely_factors: character vector of likely factors
   
    recipe(formula = formula,
           data = df) %>%
        ## NB: this is dicey and should be improved, but it's a
        ## start; using K nearest neighbors to impute.
        step_knnimpute(all_predictors()) %>%
        ## convert factors to dummy
        step_dummy(one_of(likely_factors)) %>%
        ## center predictors
        step_center(all_predictors())  %>%
        ## rescale all predictors
        step_scale(all_predictors())
}
         
```


```{r}
## H/T: https://www.brodrigues.co/blog/2018-11-25-tidy_cv/

grad_class <- function(formula, likely_factors, split, id, prog_bar) {
    ## PURPOSE
    ## This is a classification function that takes formula,
    ## factor variables (meh), and split/id from bootstrapped dataset; then
    ##
    ## (1) fits model
    ## (2) generates predictions
    ## (3) outputs to results dataframe

    prog_bar$tick()$print()
    
    ## pull training (analysis) dataset from split
    analysis_set <- analysis(split)
   
    ## prep analysis dataset using recipe function above
    analysis_prep <- prep(recipe_function(analysis_set, 
                                          formula,
                                          likely_factors),
                          training = analysis_set)
   
    ## output model matrix and outcome
    analysis_processed <- bake(analysis_prep, new_data = analysis_set)

    ## fit logit regression (due to binary outcome)
    logit_fit <- logistic_reg(mode = "classification") %>%
        set_engine("glm") %>%
        fit(formula = formula,
            data = analysis_processed)
   
    ## prep testing (assessment) dataset
    assessment_set <- assessment(split)

    ## prep assessment dataset using recipe function above
    assessment_prep <- prep(recipe_function(assessment_set,
                                            formula,
                                            likely_factors), 
                            testing = assessment_set)

    ## output model matrix and outcome
    assessment_processed <- bake(assessment_prep, new_data = assessment_set)
   
    ## return: output results as a tibble
    return(tibble("id" = id,
                  "truth" = assessment_processed[["ba_complete"]]) %>%
           bind_cols(predict(logit_fit,
                             new_data = assessment_processed,
                             type = "prob")))%>%
           bind_cols(predict(logit_fit,
                             new_data = assessment_processed,
                             type="class"))
}
```

```{r}

## DONT RUN UNLESS YOU HAVE TO

if (unzip_again || !file.exists(file.path(cddir, results_rds))) {

pb <- progress_estimated(dim(validation_data)[1]) ## Progress across elements of validation data

results <- map2_df(.x = validation_data[["splits"]],
                   .y = validation_data[["id"]],
                   ~ grad_class(formula = ba_complete_formula,
                                likely_factors = likely_factors,
                                split = .x, 
                                id = .y))


write_rds(results,path=file.path(cddir, results_rds))
} else
{
read_rds(path=file.path(cddir, results_rds))    
} 

```

```{r}
## get AUC across bootstraps
auc <- results %>%
    group_by(id) %>%
    roc_auc(truth = truth,
            prediction = .pred_Completed,
            )
```

```{r dens_auc}
## plot distribution of AUC values
g <- ggplot(auc, aes(x = .estimate)) +
    geom_density(fill = "lightblue", alpha = .5) +
    labs(x = "AUC",
         y = "Density")

## print in document
g
```

